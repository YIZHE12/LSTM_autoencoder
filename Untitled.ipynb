{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_data(file,n_timestep,n_features):\n",
    "    X = pd.read_csv(file)\n",
    "    \n",
    "    X_data = X.values[:,-1-n_features:-1]\n",
    "    X_data = StandardScaler().fit_transform(X_data) \n",
    "    X_data = MinMaxScaler().fit_transform(X_data) \n",
    "    n_examples = int(np.shape(X_data)[0]/n_timestep)\n",
    "    \n",
    "    X_data = np.reshape(X_data, (n_examples,n_timestep,n_features))\n",
    "    \n",
    "    print (np.shape(X_data))\n",
    "    return (X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yizhe\\AppData\\Local\\conda\\conda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\yizhe\\AppData\\Local\\conda\\conda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3810, 128, 10)\n"
     ]
    }
   ],
   "source": [
    "file = 'Data/X_train.csv'\n",
    "n_timestep = 128\n",
    "n_features = 10\n",
    "\n",
    "sequence = pre_data(file,n_timestep,n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128, 100)          80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 128, 10)           1010      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 10)           0         \n",
      "=================================================================\n",
      "Total params: 125,810\n",
      "Trainable params: 125,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2667 samples, validate on 1143 samples\n",
      "Epoch 1/2000\n",
      "2667/2667 [==============================] - 5s 2ms/step - loss: 0.3237 - acc: 0.0058 - val_loss: 0.3375 - val_acc: 0.0086\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00865, saving model to weights_best.hdf5\n",
      "Epoch 2/2000\n",
      "2667/2667 [==============================] - 2s 595us/step - loss: 0.3180 - acc: 0.0038 - val_loss: 0.3312 - val_acc: 0.0060\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00865\n",
      "Epoch 3/2000\n",
      "2667/2667 [==============================] - 2s 599us/step - loss: 0.3119 - acc: 0.0010 - val_loss: 0.3248 - val_acc: 0.0036\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00865\n",
      "Epoch 4/2000\n",
      "2667/2667 [==============================] - 2s 629us/step - loss: 0.3059 - acc: 5.2142e-04 - val_loss: 0.3186 - val_acc: 0.0037\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00865\n",
      "Epoch 5/2000\n",
      "2667/2667 [==============================] - 2s 612us/step - loss: 0.2999 - acc: 7.4405e-04 - val_loss: 0.3124 - val_acc: 0.0041\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00865\n",
      "Epoch 6/2000\n",
      "2667/2667 [==============================] - 2s 619us/step - loss: 0.2939 - acc: 0.0013 - val_loss: 0.3062 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.00865 to 0.01076, saving model to weights_best.hdf5\n",
      "Epoch 7/2000\n",
      "2667/2667 [==============================] - 2s 615us/step - loss: 0.2880 - acc: 0.0053 - val_loss: 0.3000 - val_acc: 0.0201\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.01076 to 0.02006, saving model to weights_best.hdf5\n",
      "Epoch 8/2000\n",
      "2667/2667 [==============================] - 2s 606us/step - loss: 0.2820 - acc: 0.0143 - val_loss: 0.2937 - val_acc: 0.0414\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.02006 to 0.04139, saving model to weights_best.hdf5\n",
      "Epoch 9/2000\n",
      "2667/2667 [==============================] - 2s 606us/step - loss: 0.2759 - acc: 0.0320 - val_loss: 0.2872 - val_acc: 0.0689\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.04139 to 0.06890, saving model to weights_best.hdf5\n",
      "Epoch 10/2000\n",
      "2667/2667 [==============================] - 2s 631us/step - loss: 0.2695 - acc: 0.1129 - val_loss: 0.2804 - val_acc: 0.1127\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.06890 to 0.11268, saving model to weights_best.hdf5\n",
      "Epoch 11/2000\n",
      "2667/2667 [==============================] - 2s 611us/step - loss: 0.2629 - acc: 0.1479 - val_loss: 0.2732 - val_acc: 0.1689\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.11268 to 0.16892, saving model to weights_best.hdf5\n",
      "Epoch 12/2000\n",
      "2667/2667 [==============================] - 2s 603us/step - loss: 0.2558 - acc: 0.1759 - val_loss: 0.2655 - val_acc: 0.2097\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.16892 to 0.20969, saving model to weights_best.hdf5\n",
      "Epoch 13/2000\n",
      "2667/2667 [==============================] - 2s 615us/step - loss: 0.2483 - acc: 0.1850 - val_loss: 0.2572 - val_acc: 0.2259\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.20969 to 0.22587, saving model to weights_best.hdf5\n",
      "Epoch 14/2000\n",
      "2667/2667 [==============================] - 2s 621us/step - loss: 0.2401 - acc: 0.1908 - val_loss: 0.2481 - val_acc: 0.2305\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.22587 to 0.23053, saving model to weights_best.hdf5\n",
      "Epoch 15/2000\n",
      "2667/2667 [==============================] - 2s 603us/step - loss: 0.2311 - acc: 0.1944 - val_loss: 0.2380 - val_acc: 0.2327\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.23053 to 0.23265, saving model to weights_best.hdf5\n",
      "Epoch 16/2000\n",
      "2667/2667 [==============================] - 2s 611us/step - loss: 0.2212 - acc: 0.1956 - val_loss: 0.2269 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.23265 to 0.23327, saving model to weights_best.hdf5\n",
      "Epoch 17/2000\n",
      "2667/2667 [==============================] - 2s 609us/step - loss: 0.2102 - acc: 0.1955 - val_loss: 0.2144 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.23327\n",
      "Epoch 18/2000\n",
      "2667/2667 [==============================] - 2s 609us/step - loss: 0.1978 - acc: 0.1955 - val_loss: 0.2001 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.23327\n",
      "Epoch 19/2000\n",
      "2667/2667 [==============================] - 2s 601us/step - loss: 0.1836 - acc: 0.1955 - val_loss: 0.1837 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.23327\n",
      "Epoch 20/2000\n",
      "2667/2667 [==============================] - 2s 623us/step - loss: 0.1673 - acc: 0.1955 - val_loss: 0.1645 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.23327\n",
      "Epoch 21/2000\n",
      "2667/2667 [==============================] - 2s 609us/step - loss: 0.1482 - acc: 0.1955 - val_loss: 0.1421 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.23327\n",
      "Epoch 22/2000\n",
      "2667/2667 [==============================] - 2s 598us/step - loss: 0.1262 - acc: 0.1955 - val_loss: 0.1160 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.23327\n",
      "Epoch 23/2000\n",
      "2667/2667 [==============================] - 2s 608us/step - loss: 0.1005 - acc: 0.1955 - val_loss: 0.0868 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.23327\n",
      "Epoch 24/2000\n",
      "2667/2667 [==============================] - 2s 603us/step - loss: 0.0780 - acc: 0.1982 - val_loss: 0.0846 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.23327\n",
      "Epoch 25/2000\n",
      "2667/2667 [==============================] - 2s 615us/step - loss: 0.0813 - acc: 0.1955 - val_loss: 0.0932 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.23327\n",
      "Epoch 26/2000\n",
      "2667/2667 [==============================] - 2s 603us/step - loss: 0.0886 - acc: 0.1955 - val_loss: 0.0989 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.23327\n",
      "Epoch 27/2000\n",
      "2667/2667 [==============================] - 2s 598us/step - loss: 0.0933 - acc: 0.1955 - val_loss: 0.1025 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.23327\n",
      "Epoch 28/2000\n",
      "2667/2667 [==============================] - 2s 615us/step - loss: 0.0962 - acc: 0.1955 - val_loss: 0.1044 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.23327\n",
      "Epoch 29/2000\n",
      "2667/2667 [==============================] - 2s 609us/step - loss: 0.0975 - acc: 0.1955 - val_loss: 0.1051 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.23327\n",
      "Epoch 30/2000\n",
      "2667/2667 [==============================] - 2s 609us/step - loss: 0.0978 - acc: 0.1955 - val_loss: 0.1049 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.23327\n",
      "Epoch 31/2000\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_timestep,n_features)))\n",
    "model.add(RepeatVector(n_timestep)) # repeat the same output for n_timestep times (upsampling)\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.add(Activation('tanh'))\n",
    "model.summary()\n",
    "optimizer_Adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer_Adam, loss='mse', metrics=['accuracy'])\n",
    "## try the lost of binary_crossentropy after normalize the data to 0-1\n",
    "# fit model\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# if want to save whole model use h5\n",
    "mc= ModelCheckpoint(\"weights_best.hdf5\", monitor='val_acc', verbose=1, \\\n",
    "                    save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit(sequence, sequence, epochs=2000, \\\n",
    "                    batch_size =1000, validation_split = 0.3, \\\n",
    "                    verbose=1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "line1, = ax.plot(history.history['loss'], color='blue', lw=2)\n",
    "line, = ax.plot(history.history['val_loss'], color='green', lw=2)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('model loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights_best.hdf5\")\n",
    "test_ex = sequence[48,:,:]\n",
    "test_ex = np.expand_dims(test_ex, axis = 0)\n",
    "y_pred = model.predict(test_ex)\n",
    "\n",
    "k = 3\n",
    "plt.subplot(121)\n",
    "plt.plot(np.squeeze(test_ex[:,:,k]))\n",
    "plt.subplot(122)\n",
    "plt.plot(np.squeeze(y_pred[:,:,k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the encoder LSTM as the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sne = 7000\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.read_csv('Data/y_train.csv')\n",
    "df_tsne['x-tsne'] = tsne_results[:,0]\n",
    "df_tsne['y-tsne'] = tsne_results[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.scatterplot(x=\"x-tsne\", y=\"y-tsne\", hue=\"surface\",\n",
    "                      data=df_tsne, alpha = 0.8)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
