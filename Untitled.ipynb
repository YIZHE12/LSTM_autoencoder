{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_data(file,n_timestep,n_features):\n",
    "    X = pd.read_csv(file)\n",
    "    \n",
    "    X_data = X.values[:,-1-n_features:-1]\n",
    "    X_data = StandardScaler().fit_transform(X_data) \n",
    "    X_data = MinMaxScaler().fit_transform(X_data) \n",
    "    n_examples = int(np.shape(X_data)[0]/n_timestep)\n",
    "    \n",
    "    X_data = np.reshape(X_data, (n_examples,n_timestep,n_features))\n",
    "    \n",
    "    print (np.shape(X_data))\n",
    "    return (X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15240, 32, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yizhe\\AppData\\Local\\conda\\conda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\yizhe\\AppData\\Local\\conda\\conda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "file = 'Data/X_train.csv'\n",
    "n_timestep = 32\n",
    "n_features = 2\n",
    "\n",
    "sequence = pre_data(file,n_timestep,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 100)               41200     \n",
      "_________________________________________________________________\n",
      "repeat_vector_13 (RepeatVect (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 32, 100)           80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 32, 2)             202       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 2)             0         \n",
      "=================================================================\n",
      "Total params: 121,802\n",
      "Trainable params: 121,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10668 samples, validate on 4572 samples\n",
      "Epoch 1/200\n",
      "10668/10668 [==============================] - 29s 3ms/step - loss: 0.1025 - val_loss: 0.0061\n",
      "Epoch 2/200\n",
      "10668/10668 [==============================] - 26s 2ms/step - loss: 0.0019 - val_loss: 8.5139e-04\n",
      "Epoch 3/200\n",
      "10668/10668 [==============================] - 26s 2ms/step - loss: 7.2158e-04 - val_loss: 5.0813e-04\n",
      "Epoch 4/200\n",
      "10668/10668 [==============================] - 26s 2ms/step - loss: 3.5042e-04 - val_loss: 4.7252e-04\n",
      "Epoch 5/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 3.3567e-04 - val_loss: 4.6117e-04\n",
      "Epoch 6/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 3.3591e-04 - val_loss: 4.6457e-04\n",
      "Epoch 7/200\n",
      "10668/10668 [==============================] - 26s 2ms/step - loss: 3.2024e-04 - val_loss: 4.4750e-04\n",
      "Epoch 8/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 3.1104e-04 - val_loss: 4.4111e-04\n",
      "Epoch 9/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 3.0509e-04 - val_loss: 4.3208e-04\n",
      "Epoch 10/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 2.9801e-04 - val_loss: 4.5456e-04\n",
      "Epoch 11/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 2.8923e-04 - val_loss: 4.2029e-04\n",
      "Epoch 12/200\n",
      "10668/10668 [==============================] - 25s 2ms/step - loss: 2.8205e-04 - val_loss: 4.1065e-04\n",
      "Epoch 13/200\n",
      "10668/10668 [==============================] - 26s 2ms/step - loss: 2.9047e-04 - val_loss: 4.1162e-04\n",
      "Epoch 14/200\n",
      " 6950/10668 [==================>...........] - ETA: 7s - loss: 2.8552e-04"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_timestep,n_features)))\n",
    "model.add(RepeatVector(n_timestep)) # repeat the same output for n_timestep times (upsampling)\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.add(Activation('tanh'))\n",
    "model.summary()\n",
    "optimizer_Adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer_Adam, loss='mse')\n",
    "# fit model\n",
    "history = model.fit(sequence, sequence, epochs=200, batch_size =50, validation_split = 0.3, verbose=1)\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex = sequence[0,:,:]\n",
    "test_ex = np.expand_dims(test_ex, axis = 0)\n",
    "y_pred = model.predict(test_ex)\n",
    "\n",
    "k = -1\n",
    "plt.subplot(121)\n",
    "plt.plot(np.squeeze(test_ex[:,:,k]))\n",
    "plt.subplot(122)\n",
    "plt.plot(np.squeeze(y_pred[:,:,k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
